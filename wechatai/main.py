import feedparser
import requests
from bs4 import BeautifulSoup
import pymysql
from openai import OpenAI
import datetime
import time
import json
import os

# ================= å…³é”®é…ç½®åŒº =================
# 1. AI é…ç½® (å·²æ”¹ä¸ºæœ¬åœ° Ollama)
client = OpenAI(
    api_key="ollama",
    base_url="http://localhost:11434/v1"
)
MODEL_NAME = os.getenv("MODEL_NAME", "qwen2.5:7b")

# 2. æ•°æ®åº“é…ç½® - æ”¹æˆ TiDB Cloud
DB_CONFIG = {
    'host': 'gateway01.ap-southeast-1.prod.aws.tidbcloud.com',
    'port': 4000,
    'user': '4UQMmu8pBXHpYPX.root',
    'password': 'ErrvTvIZ1l1WdQ90',
    'database': 'test',
    'charset': 'utf8mb4',
    'ssl': {'ssl': True}
}

# 3. è®¢é˜…æºåˆ—è¡¨
RSS_LIST = [
    "http://localhost:4000/feeds/MP_WXS_3216386757.rss",
    "http://localhost:4000/feeds/MP_WXS_3582669377.rss",
    "http://localhost:4000/feeds/MP_WXS_3072073807.rss",
    "http://localhost:4000/feeds/MP_WXS_3509014347.rss",
    "http://localhost:4000/feeds/MP_WXS_2398020661.rss",
    "http://localhost:4000/feeds/MP_WXS_3964424679.rss",
    "http://localhost:4000/feeds/MP_WXS_3274687166.rss",
    "http://localhost:4000/feeds/MP_WXS_3229412976.rss",
    "http://localhost:4000/feeds/MP_WXS_3252128862.rss",
    "http://localhost:4000/feeds/MP_WXS_3219231991.rss",
    "http://localhost:4000/feeds/MP_WXS_3276902399.rss",
    "http://localhost:4000/feeds/MP_WXS_3935938222.rss",
    "http://localhost:4000/feeds/MP_WXS_3198215923.rss"
]

CATEGORIES = ["æŠ€æœ¯ç ”å‘ä¸çªç ´", "æ”¿ç­–æ³•è§„ä¸å¸‚åœºäº¤æ˜“", "å·¥ç¨‹é¡¹ç›®ä¸å¹¶ç½‘å®è·µ", "ä¼ä¸šåŠ¨å‘ä¸äº§ä¸šç»æµ", "åŸºç¡€çŸ¥è¯†ä¸ç§‘æ™®è§£è¯»",
              "å®‰å…¨äº‹ä»¶ä¸äº‹æ•…å¤„ç†", "å…¶ä»–"]


# ============================================

def get_full_text_from_wechat(url):
    """ã€è·³è½¬æŠ“å–ã€‘ç›´æ¥å»å¾®ä¿¡å®˜ç½‘æŠ“æ­£æ–‡"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        content = soup.find('div', id='js_content')
        return content.get_text(strip=True) if content else ""
    except:
        return ""


def generate_simple_summary(title, content):
    """ç”Ÿæˆç®€å•æ‘˜è¦ï¼ˆä¸è°ƒç”¨AIï¼Œæé«˜é€Ÿåº¦ï¼‰"""
    clean_content = content.replace('\n', ' ').replace('\r', ' ')
    if len(clean_content) > 200:
        return clean_content[:200] + "..."
    return clean_content if clean_content else "ç‚¹å‡»æŸ¥çœ‹åŸæ–‡"


def generate_summary_only(title, content):
    try:
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ–‡ç« æ‘˜è¦åŠ©æ‰‹ï¼Œåªè¾“å‡ºçº¯æ–‡æœ¬æ‘˜è¦"},
                      {"role": "user", "content": f"ä¸ºä»¥ä¸‹æ–‡ç« ç”Ÿæˆ3å¥è¯æ‘˜è¦ï¼š\næ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:800]}"}],
            max_tokens=150
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"  âŒ AI æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        return "ç‚¹å‡»æŸ¥çœ‹åŸæ–‡"


def call_ai_for_classification_and_summary(title, content):
    try:
        content_preview = content[:500] if len(content) > 500 else content

        prompt = f"""ä½œä¸ºå‚¨èƒ½è¡Œä¸šèµ„æ·±ç ”ç©¶å‘˜ï¼Œè¯·å°†ä»¥ä¸‹æ–‡ç« å½’ç±»åˆ°ä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š{CATEGORIES}

åˆ¤æ–­ä¾æ®ï¼š
- æŠ€æœ¯ç ”å‘ä¸çªç ´ï¼šåŒ…å«å‚¨èƒ½æŠ€æœ¯ç ”å‘ã€åˆ›æ–°ã€ä¸“åˆ©ã€å®éªŒè¿›å±•ç­‰å†…å®¹
- æ”¿ç­–æ³•è§„ä¸å¸‚åœºäº¤æ˜“ï¼š**ä¸“æŒ‡ç”µåŠ›è¡Œä¸š**çš„æ”¿ç­–æ–‡ä»¶ã€å¸‚åœºæœºåˆ¶ã€äº¤æ˜“è§„åˆ™ã€ç”µä»·æ”¿ç­–ã€å¹¶ç½‘ç®¡ç†ç­‰å†…å®¹
- ä¼ä¸šåŠ¨å‘ä¸äº§ä¸šç»æµï¼šåŒ…å«å‚¨èƒ½ä¼ä¸šæ’åã€äº§é‡æ•°æ®ã€è´¢åŠ¡ä¿¡æ¯ã€å¸‚åœºåˆ†æç­‰å†…å®¹  
- å·¥ç¨‹é¡¹ç›®ä¸å¹¶ç½‘å®è·µï¼šåŒ…å«å‚¨èƒ½é¡¹ç›®å»ºè®¾ã€å¹¶ç½‘è¿è¡Œã€å·¥ç¨‹å®æ–½ç­‰å†…å®¹
- åŸºç¡€çŸ¥è¯†ä¸ç§‘æ™®è§£è¯»ï¼šåŒ…å«å‚¨èƒ½æŠ€æœ¯åŸç†ã€å…¥é—¨æ•™ç¨‹ã€ç§‘æ™®çŸ¥è¯†ç­‰å†…å®¹
- å®‰å…¨äº‹ä»¶ä¸äº‹æ•…å¤„ç†ï¼šåŒ…å«å‚¨èƒ½å®‰å…¨äº‹æ•…ã€åº”æ€¥å¤„ç†ã€é£é™©ç®¡æ§ç­‰å†…å®¹

æ–‡ç« æ ‡é¢˜ï¼š{title}
æ–‡ç« å¼€å¤´å†…å®¹ï¼š{content_preview}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{"category": "åˆ†ç±»åç§°", "summary": "3å¥æ‘˜è¦"}}"""

        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåªè¾“å‡ºJSONå¹¶ä¸¥æ ¼éµå®ˆé¢„è®¾åˆ†ç±»åçš„æœºå™¨äºº"},
                {"role": "user", "content": prompt}
            ],
            response_format={'type': 'json_object'}
        )

        raw_response = response.choices[0].message.content
        print(f"    ğŸ¤– AIåŸå§‹å“åº”: {raw_response}")

        res = json.loads(raw_response)
        ai_category = res.get('category', 'å…¶ä»–').strip()
        if ai_category not in CATEGORIES:
            print(f"    âš ï¸ AIè¿”å›æ— æ•ˆåˆ†ç±»: '{ai_category}'ï¼Œå½’ä¸ºå…¶ä»–")
            ai_category = "å…¶ä»–"
        return res.get('summary', 'ç‚¹å‡»æŸ¥çœ‹åŸæ–‡').strip(), ai_category
    except Exception as e:
        print(f"  âŒ AI è°ƒç”¨å¤±è´¥: {e}")
        return "AIåˆ†æå¤±è´¥", "å…¶ä»–"


def analyze_article(title, content):
    print(f"  ğŸ¤– æ­£åœ¨åˆ†ææ–‡ç« : {title[:20]}...")

    title_clean = title.replace(" ", "").lower()
    summary = "ç‚¹å‡»æŸ¥çœ‹åŸæ–‡"

    print(f"    ğŸ” æ¸…æ´—åæ ‡é¢˜: {title_clean}")

    safety_keywords = ["äº‹æ•…", "çˆ†ç‡ƒ", "çˆ†ç‚¸", "ç«ç¾", "ä¼¤äº¡", "å®‰å…¨", "éšæ‚£", "æ•´æ”¹", "é€šæŠ¥"]
    if any(k in title_clean for k in safety_keywords):
        category = "å®‰å…¨äº‹ä»¶ä¸äº‹æ•…å¤„ç†"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸš¨ å®‰å…¨äº‹ä»¶å‘½ä¸­: {category}")
        return summary, category

    company_keywords = ["ç›ˆåˆ©", "è´¢æŠ¥", "ä¸Šå¸‚", "å¹¶è´­", "æ’å", "top", "äº§é‡", "é”€é‡", "åŠ¨æ€", "å¸‚åœºä»½é¢"]
    if any(k in title_clean for k in company_keywords):
        category = "ä¼ä¸šåŠ¨å‘ä¸äº§ä¸šç»æµ"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸ’¼ ä¼ä¸šåŠ¨å‘å‘½ä¸­: {category}")
        return summary, category

    science_keywords = ["ç§‘æ™®", "å…¥é—¨", "æ•™ç¨‹", "æ•™å­¦", "å­¦ä¹ ", "æ–¹æ³•", "æŠ€å·§", "è¯¦è§£", "åŸç†", "å›¾è§£", "è§£å†³åŠæ³•",
                        "æ€»ç»“", "å¸¸è§é—®é¢˜"]
    if any(k in title_clean for k in science_keywords):
        category = "åŸºç¡€çŸ¥è¯†ä¸ç§‘æ™®è§£è¯»"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸ“š ç§‘æ™®ç±»å‘½ä¸­: {category}")
        return summary, category

    policy_keywords = ["æ”¿ç­–", "æ³•è§„", "æ ‡å‡†", "è¡¥è´´", "ç®¡ç†åŠæ³•", "ç”µä»·", "å¸‚åœºäº¤æ˜“"]
    if any(k in title_clean for k in policy_keywords):
        category = "æ”¿ç­–æ³•è§„ä¸å¸‚åœºäº¤æ˜“"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸ“‹ æ”¿ç­–æ³•è§„å‘½ä¸­: {category}")
        return summary, category

    tech_keywords = ["ç ”å‘", "æŠ€æœ¯", "çªç ´", "åˆ›æ–°", "ä¸“åˆ©", "æœ€æ–°è¿›å±•"]
    if any(k in title_clean for k in tech_keywords):
        category = "æŠ€æœ¯ç ”å‘ä¸çªç ´"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸ”¬ æŠ€æœ¯ç ”å‘å‘½ä¸­: {category}")
        return summary, category

    project_keywords = ["é¡¹ç›®", "å·¥ç¨‹", "å»ºè®¾", "å¹¶ç½‘", "mw", "gw", "å‚¨èƒ½ç”µç«™", "ç¤ºèŒƒ"]
    if any(k in title_clean for k in project_keywords):
        category = "å·¥ç¨‹é¡¹ç›®ä¸å¹¶ç½‘å®è·µ"
        summary = generate_simple_summary(title, content)
        print(f"    ğŸ—ï¸ å·¥ç¨‹é¡¹ç›®å‘½ä¸­: {category}")
        return summary, category

    print(f"    ğŸ¤” å…³é”®è¯æœªå‘½ä¸­ï¼Œè°ƒç”¨AIè¿›è¡Œæ·±åº¦åˆ†æ...")
    summary, category = call_ai_for_classification_and_summary(title, content)

    if category in CATEGORIES and category != "åˆ†ç±»åç§°":
        print(f"    ğŸ¤– é‡‡ç”¨AIåˆ†ç±»: {category}")
    else:
        category = "å…¶ä»–"
        print(f"    âš ï¸ AIåˆ†ç±»æ— æ•ˆæˆ–æœªè¯†åˆ«ï¼Œå½’ä¸ºå…¶ä»–")

    return summary, category


def article_exists_in_db(title):
    """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å­˜åœ¨äº TiDB æ•°æ®åº“ä¸­"""
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        with conn.cursor() as cursor:
            sql = "SELECT COUNT(*) FROM articles WHERE title = %s"
            cursor.execute(sql, (title,))
            result = cursor.fetchone()
            return result[0] > 0
    except Exception as e:
        print(f"  âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
        return False
    finally:
        if conn:
            conn.close()


def save_to_db(data):
    """ä¿å­˜å•æ¡æ–‡ç« æ•°æ®åˆ° TiDB Cloud"""
    conn = None
    try:
        conn = pymysql.connect(**DB_CONFIG)
        with conn.cursor() as cursor:
            sql = """INSERT INTO articles
                         (title, link, author, publish_date, summary, category, raw_content)
                     VALUES (%s, %s, %s, %s, %s, %s, %s)"""
            cursor.execute(sql, data)
        conn.commit()
        print(f"  âœ… [å…¥åº“æˆåŠŸ]: {data[0][:20]}...")
    except Exception as e:
        print(f"  âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
    finally:
        if conn:
            conn.close()


def run():
    print(f"--- ğŸš€ TiDB Cloud AI è‡ªåŠ¨åŒ–é‡‡é›†å¯åŠ¨: {datetime.datetime.now()} ---")

    total_directories = len(RSS_LIST)
    total_processed_articles = 0
    skipped_articles = 0

    print("  ğŸ“¥ é‡‡ç”¨å¢é‡æ›´æ–°æ¨¡å¼ï¼Œä¿ç•™å†å²æ•°æ®")

    for index, rss_url in enumerate(RSS_LIST, 1):
        print(f"\nğŸ“¡ è¯»å–ç›®å½• [{index}/{total_directories}]: {rss_url}")

        directory_article_count = 0
        directory_skipped_count = 0

        try:
            feed = feedparser.parse(rss_url)

            for entry in feed.entries:
                print(f"ğŸ“– æ£€æŸ¥: {entry.title}")

                if article_exists_in_db(entry.title):
                    print(f"  â­ï¸  æ–‡ç« å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    directory_skipped_count += 1
                    skipped_articles += 1
                    continue

                text_only = get_full_text_from_wechat(entry.link)
                if len(text_only) < 100:
                    print(f"  âš ï¸ æ— æ³•è·å–æ­£æ–‡ï¼Œè·³è¿‡")
                    continue

                summary, category = analyze_article(entry.title, text_only)

                save_to_db((
                    entry.title,
                    entry.link,
                    "å…¬ä¼—å·",
                    datetime.datetime.now(),
                    summary,
                    category,
                    text_only
                ))

                directory_article_count += 1
                total_processed_articles += 1

            print(f"  ğŸ“Š è¯¥ç›®å½•å¤„ç†: {directory_article_count} ç¯‡æ–°å¢, {directory_skipped_count} ç¯‡è·³è¿‡")

        except Exception as e:
            print(f"  âŒ è¯»å–ç›®å½•å¤±è´¥: {e}")
            continue

    print(f"\n--- âœ¨ ä»»åŠ¡å…¨éƒ¨å®Œæˆ ---")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   â€¢ æ€»ç›®å½•æ•°: {total_directories}")
    print(f"   â€¢ æœ¬æ¬¡æ–°å¢æ–‡ç« æ•°: {total_processed_articles}")
    print(f"   â€¢ è·³è¿‡é‡å¤æ–‡ç« æ•°: {skipped_articles}")
    print(f"   â€¢ å¹³å‡æ¯ç›®å½•æ–°å¢: {total_processed_articles / total_directories:.1f} ç¯‡")


if __name__ == "__main__":
    run()
